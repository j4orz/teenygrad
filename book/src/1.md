<img src="./assets/euclid.jpg" loading="lazy"></br>
<small>*Euclid Instructing His Pupils in The School of Athens, Raffaello Sanzio da Urbino 1509-1511.*</small></br>
<small>*This book is best viewed on a desktop.*</small>

# I. Elements of Networks</br>

Although separated by 2000 years,
the discipline of programming originally practiced in Silicon Valley shares the same dream of mathematics in Ancient Greece:
using *discrete* and *deterministic* descriptions to model reality.
Although mathematicians prove properties about mathematical objects with theorems
whereas programmers evaluate the values of said objects with programs,
what the two traditions share in common is the activity of modeling *discrete objects* with *deterministic descriptions*
which in turn, are created for them by toolsmiths referred to as logicians and language implementors
‚Äî the mathematician describes the world with a logic, and the programmer does so with a programming language.

<!-- As always, history is a guide, and in fact the transition in both *object* and *function* has already been made by our mathematical cousins. -->
However, for the programmer interested in learning the skills necessary for the statistical learning approach to artificial intelligence,
they must make a transition to programming from *discrete data* using *deterministic functions* to that of *continuous data* $x$ using *stochastic functions* $f(x)$.
Although you will encounter many new concepts in your journey up ahead, there are also many similarities with the art of programming that you already know.
Thus far in software 1.0, you've composed programs with data structures and algorithms<span class="sidenote-number"></span><span class="sidenote">*Perhaps with classic canon like [Algorithms + Data Structures = Programs](https://archive.org/details/algorithmsdatast0000wirt)*</span>,
which in turn are composed of data and function.
The following is a software 2.0 modified excerpt from *The Structure and Interpretation of Computer Programs*, a book which is part of the programmer's classic cannon:

> We are about to study the idea of a computational process. Computational processes are abstract beings that inhabit computers. As they evolve, processes manipulate other abstract things called data. The evolution of a process is directed by a pattern of ~~rules called a program~~ *probabilities called a model*. People create ~~programs to direct processes~~ *models to recover processes*. In effect, we conjure the spirits of the computer with our spells.</br></br>
A computational process is indeed much like a sorcerer's idea of a spirit. It cannot be seen or touched. It is not composed of matter at all. However, it is very real. It can perform intellectual work. It can answer questions. It can affect the world by disbursing money at a bank or by controlling a robot arm in a factory. The programs we use to conjure processes are like a sorcerer's spells. They are carefully composed from ~~symbolic~~ *numerical* expressions in arcane and esoteric programming languages that prescribe the tasks we want our processes to perform.A computational process, in a correctly working computer, ~~executes programs precisely and accurately~~ *recovers models stochastically and noisily*. Thus, like the sorcerer's apprentice, novice programmers must learn to understand and to anticipate the consequences of their conjuring. Even small errors (usually called bugs or glitches) in programs can have complex and unanticipated consequences.

So in [1.1 Continuously Stochastic *Data* $\mathbf{x}$ with `numpy`](), we will first learn about programming *data* $\mathbf{x}$ of software 2.0
by representing types continuously with coordinates, distributing truth stochastically with probability,
and then recovering said distributions with maximum likelihood estimation.
Then, in [1.2 Statistical Learning of *Continuously Stochastic Maps* with `numpy`](),
and [1.3 Statistical Learning of Continuously Stochastic *Classifying* Maps with `HIPS/autograd`]()
we will learn how to recover *stochastic functions* $y=f(\mathbf{x})$ by
automatically evaluating their derivatives $f'(x)$ with `HIPS/autograd`
<span class="sidenote-number"></span><span class="sidenote">*[HIPS/autograd](https://github.com/HIPS/autograd) along with [torch-autograd](https://github.com/twitter-archive/torch-autograd) were the first numerical libraries to implement automatic differentiation in the ecosystems of productive programming languages like Python's `numpy` and Lua's `torch`, which ultimately inspired PyTorch. HIPS/autograd is used in part one of the book to emphasize the automatic differentiation abstraction was implemented before the heavy GPU acceleration PyTorch provided. For more information Soumith's [PyTorch's design origins](https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html) is an excellent read.*</span>
and using it's direction of steepest descent to iteratively optimize the loss with stochastic gradient descent.


From there, readers will unravel the mystery in how the internals of `numpy` and `torch` are implemented
by building `teenygrad` from scratch ‚Äî after three chapters we will have built our very own CPU accelerated multidimensional array with automatic differentiation
which will be able to run the generalized linear models developed throughout the first three chapters.
So in
[1.4 Programming `teenygrad` *Level -1*: Tensor, the Multidimensional Array](#14-programming-teenygrad-level--1-high-dimensional-data-and-function),
`teenygrad.Tensor` is implemented in Python with strides that virtualize arbitrary n-dimensinal shapes to a 1-dimensional physical array.
Then, in
[1.5 Programming `teenygrad` *Level 0*: Accelerating the `BLAS` on CPU](#15-programming-teenygrad-level-0-accelerating-the-blas-on-cpu),
the basic linear algebra operations on the Tensor are accelerated on the CPU
by changing the host language from Python to Rust
in order to translate programs into native machine code called assembly with the `rustc` compiler
which enables us to analyze and tune the performance for the underlying processor's pipelines and memory hierarchies
<span class="sidenote-number"></span><span class="sidenote">*Berkeley's opensource educational RISC-V cores [`sodor`](https://github.com/ucb-bar/riscv-sodor) and [`BOOM`](https://github.com/riscv-boom/riscv-boom) cores along with Tenstorrent's RVV extension fork [`ocelot`](https://github.com/tenstorrent/riscv-ocelot) will be used to introduce instruction sets and microarchitecture,
but x86 machines like Intel's Xeon and AMD's EPYC will be used in order to achieve server-class performance.*</span>
Finally, in [1.6 Programming `teenygrad` *Level 1*: Optimization and Differentiation](#16-programming-teenygrad-level-1-optimization-and-differentiation),
development moves back up to Python in order to implement automatic differentiation with `Tensor.backward()`.

From there, [part two](./2.md) of the SITP book takes you on the next step in the journey
and then evolving `teenygrad`'s implementation to support the same neural network primitives, with GPU acceleration.
by increasing the expressivity of the models that are trained from generalized linear models to deep neural networks with `torch`.
<span class="sidenote-number"></span><span class="sidenote">*In part three, `teenygrad` will be updated once again for SITP's ultimate capstone project which modifies the eager mode interpreter implementation to a graph mode compiler in order to support the [nanogpt]() and [nanochat]() models. This final version of `teenygrad` will share 80% of the abstractions with the pure fusion compiler called [tinygrad](), providing a bridge to the frontier of deep learning systems.*</span>

<!-- This involves moving from the "productive" programming language of Python used thus far
to a "systems"<span class="sidenote-number"></span><span class="sidenote">*"productive" and "systems" with quotes because taxonifying languages into paradigms such as "functional", "imperative", "object-oriented", "web", "systems" is non-sensical. To learn more, read Krishnamurthi's [Teaching Programming Languages in a Post-Linnaean Age](https://cs.brown.edu/~sk/Publications/Papers/Published/sk-teach-pl-post-linnaean/) from SIGPLAN 2008.*</span> programming language of Rust. -->

## Table of Contents
- [1.1 Statistical Inference of Continuously Stochastic *Data* with `numpy`](#11-statistical-inference-of-stochastic-data-with-python)
  - [1.1.1 Distributing Truth Stochastically with Probabilities of a *Probability Space*]()
  - [1.1.2 Probabilities on Equally Likely Events with Elementary Combinatorics]()
  - [1.1.3 Probabilities on Events with Rules: Sum Rule, Product Rule, Bayes Rule]()
  - [1.1.4 Random Variables with their Distributions, Expectations, and Variance]()
  - [1.1.5 Distributions and their Parameters from Data with Maximum Likelihood Estimation]()
- [1.2 Statistical Learning of Continuously Stochastic *Functions* with `numpy`](#12-statistical-learning-of-continuously-stochastic-function-with-numpy)
  - [1.2.1 Representing Types in Higher Dimensions with the Coordinates of a *Vector Space*]()
  - [1.2.2 Linear Transformations with Matrix-Vector Multiplication]()
  - [1.2.3 Predicting Scalars with Linear Regression and Mean Squared Error Loss]()
  - [1.2.4 Fitting a Line Directly with Normal Equations]()
  - [1.2.5 Predicting Categories with Logistic Regression and Cross Entropy Loss]()
  - [1.2.6 Fitting a Line Iteratively with Gradient Descent's Derivatives of a *Hilbert Space*]()
  - [1.2.7 Generalized Linear Models, Generalization, Bias-Variance Tradoff, and Double Descent]()
- [1.3 Programming CPU Accelerated Basic Linear Algebra Subroutines with `teenygrad.Tensor`](#13-programming-cpu-accelerated-basic-linear-algebra-subroutines-with-teenygradtensor)
  - [1.3.1 The Simplest `Tensor` Implementation with Nested `array`s]()
  - [1.3.2 Virtualizing ND Logical Shapes to 1D Physical Storage with Strides]()
  - [1.3.3 Implementing Basic Linear Algebra Subroutines in Python: `AXPY`, `GEMV` and `GEMM`]()
  - [1.3.4 Accelerating the `BLAS` with Processor Pipelines and Memory Hierarchies]()
  - [1.3.5 Accelerating Computation with *Instruction Level Parallelism* via Loop Unrolling]()
  - [1.3.6 Accelerating Computation with *Data Level Parallelism* via Vector/Matrix Extensions]()
  - [1.3.7 Accelerating Communication with *Memory Hierarchies* via Register and Cache Blocking]()
  - [1.3.8 Accelerating Computation with *Thread Level Parallelism*]()

## 1.1 Statistical Inference of Stochastic *Data* with `numpy`
<small>[$\hookleftarrow$ Table of Contents](#table-of-contents)</small>

### 1.1.1 Distributing Truth Stochastically with Probabilities of a Probability Space
Throughout the history of mathematics, it was uncertain whether
probabilities could be modeled with existing mathemtical formalisms,
because by default, mathematical reasoning was understood to be *deterministic*
where some proposition $A$ was judged to be either true or false,
rather than some stochastic *experiment* with many possible states.
It wasn't until 1933 where Kolmogorov formalized *probability theory*[^1] with the following set-theoretic construction:
1. the set of all possible states (*outcomes*) of the experiment is the *sample space* $\Omega$
2. the set of all possible *subsets* of the *sample space* is the *event space* $\mathcal{F} \subseteq \Omega$
3. a *measure* $\mathbb{P}:\mathcal{F}\to[0,1]$ which maps all events to some real between $0$ and $1$

```bob                      
   .---------------.
   |               |                      | 1
   |      œâ‚ÇÖ  ‚Ä¢    |                      |
   |  .-------.    |                      |
   |  |   B   |    |                      |
   |  | œâ‚ÇÑ ‚Ä¢  |----|--------------------->| ‚Ñô(B)
   |  | œâ‚ÇÉ ‚Ä¢  |    |                      |
   |  '-------'    |   Probability Law    |
   |               |   ‚Ñô:  2^Œ© -> [0,1]   |
   |  .-------.    |                      |
   |  |   A   |    |                      |
   |  | œâ‚ÇÇ ‚Ä¢  |----|--------------------->| ‚Ñô(A)
   |  | œâ‚ÇÅ ‚Ä¢  |    |                      |
   |  '-------'    |                      |
   |               |                      |
   '---------------'                      | 0
    Sample Space Œ©
    Event Space  2^Œ©

    Figure n
```
<u>Two Interpretations</u></br>
Together this triplet of two sets and one function $(\Omega, \mathcal{F}, \mathbb{P})$ defines a *probability space*.
Focusing on the map $\mathbb{P}$, although it formalizes the notions of "chance", "likeliness", and "probability"
as a numerical assignment between $0$ and $1$ on every event in the event space,
there are two possible semantic interpretations of the value *returned* by this function when modeling real-world stochastic phenomena
‚Äî namely that of the so-called *frequentist* and *bayesian* interpretation of probability.
The former interprets probabilities (the values returned by the probability measure)
as an "objective" *belief* where the value returned is the *count* of the event in which an infinite sequence of repeated trials converges to.
That is,
$$\mathbb{P}(E) = \lim\limits_{n\to\infty} \frac{\text{count}(E)}{n}$$

so for instance, the most trivial example is an experiment of rolling a single fair coin
with a probability assignment of $\mathbb{P}(\{\text{head}\})=0.5$.
The experiment can be repeated many times,
and the frequency between the count of heads and the number of trials will converge to $0.5$.
As programmers, running a simulation of such experiment is quite simple:

```python
import random

sample_space = {"üê∂", "üêï"} # dog head, dog tail
def experiment():
  outcome = random.choice(list(sample_space))
  return outcome

if __name__ == "__main__":
  n = 10_000_000
  target_e, target_e_count = {"üê∂"}, 0
  for i in range(n):
    outcome = experiment()
    if outcome in target_e: target_e_count += 1

  prob_e = len(target_e) / len(sample_space)
  freq_e = target_e_count / n
  print(f"the expected probability of {target_e} is {prob_e}")
  print(f"the actual frequency of {target_e} in {n} trials is {freq_e}")
```

Although running the simulation results in slightly different numbers,
they are all close to $0.5$ within a few precision points.
The probability assignment of $0.5$ is something that can be "objectively" verified.
To contrast, bayesian interpretation of probabilities are "subjective" beliefs
where the value returned represents the internal uncertainty of the observer.
For instance, consider an unfair die (todo..)


It's important to remember that neither interpretation is "the" correct one,
and often times the *availability of data* implies which one to use.
todo..returning to functional axiomaticization...probability is the weight of a set.

Either way, all probabilities are formally modeled by the two sets and one function of
$(\Omega, \mathcal{F}, \mathbb{P})$, with $\mathcal{F} \subseteq \Omega$ and $\mathbb{P}: \mathcal{F}\to[0,1]$
which additionally, must satisfy three axiomatic invariants[^2]:
- **non-negativity:** $\forall E \in \mathcal{F}, \mathbb{P}(E) \geq 0.$
- **normalization:** $\sum\limits_{i=0}^{\infty} \mathbb{P}(E_i)=1.$
- **additivity:**

For instance, thus far we've seen a discrete sample space,
and we'll encounter continuous ones such as $\reals$ and $\reals^n$
starting with chapter [1.1.4 Representing Types Continuously with Vector Spaces]().
But for now, the next two chapters of
[1.1.2 Probabilities on Equally Likely Events with Elementary Combinatorics]()
and [1.1.3 Probabilities on Events with Rules: Sum Rule, Product Rule, Bayes Rule]()
present different experiments to reify the abstract concept of *probability space*,
starting with those that admit themselves to *equally likely* finite sample spaces,
where $\forall o_i \in \Omega, \mathbb{P}(\{o_i\}) = \frac{1}{\vert\Omega\vert}$,
which subsequently allows the probability measure $\mathbb{P}$ to be that of a *counter*,
reducing the calculation to that of *enumerating*, *counting* the two sets of sample space and event,
and then evaluating the *ratio*:
i.e $\mathbb{P}(E)=\frac{\vert E \vert}{\vert \Omega \vert}$, which
‚Äî demonstrating with the earlier coin flip example ‚Äî
is evaluated with `len` and `/` on Python's [`set`](https://docs.python.org/3/library/stdtypes.html#types-set):
```python
sample_space, target_e = {"üê∂", "üêï"}, {"üê∂"}
prob_e = len(target_e) / len(sample_space)
print(f"the probability of {target_e} is {prob_e}")
```

### 1.1.2 Probabilities on Equally Likely Events with Elementary Combinatorics
<u>Example 1</u></br>
*Consider the sum of rolling two dice.*</br>
*What is the probability that the sum is 8?*</br>

<details>
  <summary>Solution 1: ‚ö†Ô∏è STOP and THINK, then click to reveal.</summary>
</details>

```python
sample_space = {"1Ô∏è‚É£", "2Ô∏è‚É£", "3Ô∏è‚É£", "4Ô∏è‚É£", "5Ô∏è‚É£", "6Ô∏è‚É£"} # six-sided die
event = {"2Ô∏è‚É£", "4Ô∏è‚É£", "6Ô∏è‚É£"}
prob_event = len(event) / len(sample_space)
print(f"the probability of {event} is {prob_event}")
```



<u>Example 2</u></br>
*Consider a jar with four red balls and two blue balls.*</br>
*What is the probability of choosing 1 red ball and 2 blue balls?*</br>

<details>
  <summary>Solution 2: ‚ö†Ô∏è STOP and THINK, then click to reveal.</summary>

This is the hidden spoiler content. 
It can contain more Markdown, like **bold text**, lists, or [links]()

```python
import itertools

# get all 4! = 24 permutations of 1,2,3,4 as a list:
print(list(itertools.permutations([1,2,3,4])))
# [(1, 2, 3, 4), (1, 2, 4, 3), (1, 3, 2, 4), (1, 3, 4, 2), (1, 4, 2, 3), (1, 4, 3, 2), (2, 1, 3, 4), (2, 1, 4, 3), (2, 3, 1, 4), (2, 3, 4, 1), (2, 4, 1, 3), (2, 4, 3, 1), (3, 1, 2, 4), (3, 1, 4, 2), (3, 2, 1, 4), (3, 2, 4, 1), (3, 4, 1, 2), (3, 4, 2, 1), (4, 1, 2, 3), (4, 1, 3, 2), (4, 2, 1, 3), (4, 2, 3, 1), (4, 3, 1, 2), (4, 3, 2, 1)]

# get all 3!/2! = 3 unique permutations of 1,1,2 as a set:
print(set(itertools.permutations([1,1,2])))
# {(1, 1, 2), (1, 2, 1), (2, 1, 1)}
```

```python
import itertools
# Get all ways of choosing three numbers from [1,2,3,4,5]
print(list(itertools.combinations([1,2,3,4,5], 3)))
# [(1, 2, 3), (1, 2, 4), (1, 2, 5), (1, 3, 4), (1, 3, 5), (1, 4, 5), (2, 3, 4), (2, 3, 5), (2, 4, 5), (3, 4, 5)]
```

</details>

<u>Example 3</u></br>
*Consider a deck of cards with 52 cards with 13 ranks and four suits.*</br>
*What is the probability of choosing a straight (five consecutive cards)*?</br>

<details>
  <summary>Solution 3: ‚ö†Ô∏è STOP and THINK, then click to reveal.</summary>
</details>

Because the previous examples admitted equally likely outcome spaces, the calculation of probability was reduced to that of *counting*,
which can be done via Python's builtin support for elementary combinatorics via iterators such as [`itertools.permutations`](https://docs.python.org/3/library/itertools.html#itertools.permutations)
and [`itertools.combinations`](https://docs.python.org/3/library/itertools.html#itertools.combinations).
If these builtins were not provided, the set of all permutations or combinations can be generated with *backtracking*,
‚Äî an exhaustive search (searching through the entire state space) technique that can implemented with either recursion or iteration:

```python
from typing import TypeVar, Generic
T = TypeVar('T')

def permutations(input: list[T]) -> list[list[T]]:
  return []

def combinations(input: list[T]) -> list[list[T]]:
  return []
```

If you are new to *programming*,
please refer to [HTDP Part 2: Arbitrarily Large Data](https://htdp.org/2019-02-24/part_two.html),
[DCIC Chapter 5: Lists](https://dcic-world.org/2025-08-27/part_lists.html),
and [DCIC Chapter 9: From Pyret to Python]().
If you are new to *python*, please refer to the [Python Tutorial](https://docs.python.org/3/tutorial/index.html).

### 1.1.3 Probabilities on Events with Rules: Sum Rule, Product Rule, Bayes Rule

sum rule (or, mutually exclusive), product rule (and, independance)
(footnote: sum rule, product rule with derivatives. sum types and product types with algebraic data types)

*Bayes Rule*
(Conditional probability) -> logic is a corner case

different sample spaces...
distributions


### 1.1.4 Random Variables with their Distributions, Expectations, and Variance
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/cy8r7WSuT1I?si=KHSJ5xM_fu91vkKu" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

*Bernouilli*

*Gaussian*
$$
\begin{aligned}
X &\sim \mathcal{N(\mu, \sigma)} \\
            &= 
\end{aligned}
$$

then evaluate middle $E[X]=\mu$, width $Var(X)=\sigma$
standard normla (0,1)

f(x) = cexp(x^2 / 2)


mean mu, covariance sigma
```python
import math, dataclasses

@dataclasses.dataclass
class Gaussian:
  mu: list[float]
  sigma: list[list[float]]

  def pdf(self, x:) -> float:
    return math.exp()

```

*Exponential*

### 1.1.5 Probabilities from Data with Parameter Estimation

## 1.2 Statistical Learning of Continuously Stochastic *Function* with `numpy`
<small>[$\hookleftarrow$ Table of Contents](#table-of-contents)</small>

After completing [1.1 Statistical Inference of *Stochastic Data* with `Python`](),
you now understand the core essence behind the
*statistical learning* approach to *artificial intelligence*
which is
1. the generative-like nature of *evaluating* a stochastic function
2. the recovery-like nature of *inverting* such a function

```bob

                      "sampling"/"generating" X's
                    .----------------------.
                   /                        \
                  /                          v
      .-----------.                          .-----------.
      |           |                          |           |
      |           |                          |           |
      |           |                          |           |
      |           |                          |           |
      |  Params Œ∏ |                          | Samples X |
      |           |                          |           |
      |           |                          |           |
      |           |                          |           |
      |           |                          |           |
      '-----------'                          '-----------'
                  ^                          /
                   \                        /
                    '----------------------'
                    "inferring"/"estimating" Œ∏'s

      Figure n
```

and stands in contrast to the classic way of programming which is to
specify and declare functions *up front* with explicit deterministic instructions.
The software 2.0 essence of *recovering functions from data* is more appropriate
for continuously stochastic phenomena such as language and vision.
Now, in [1.1 Statistical Inference of *Stochastic Data* with `Python`](),
we move on to not just recovering data $X \sim p$, but *maps* $Y=X \sim p$
in coordinate systems with higher dimensions.

### 1.2.1 Representing Types in Higher Dimensions with the Coordinates of a Vector Space
Probability space $(\Omega, \mathcal{F}, \mathbb{P})$, we want to "plug in" a
set of outcomes in higher dimensions..


In the traditional way of programming software 1.0, data is *discrete*.
You may have experience using (and in a data structure class, implementing)
collections such as sets, associations, sequences, trees, and graphs.
Although many of us have graduated onto these more "complex" data structures,
representing types discretely becomes problematic when it comes to modelling
*fuzzy-spectrum-like* phenomena such as *language* and *vision*.
The continous alternative to the discrete descriptions thus far is a notion
that everyone was introduced to in middle school: a *coordinate system*,
and there happens to be an "escape hatch" which all programming languages provide ‚Äî
simply representing a type with numerical `float` fields only.
Try executing the code below:

```python
from dataclasses import dataclass

@dataclass
class PhysicalCoord:
  x: float # length
  y: float # width
  z: float # height

  def __repr__(self) -> str:
    return f"({self.x}, {self.y}, {self.z})"

if __name__ == "__main__":
  fly1 = PhysicalCoord(x=3.0, y=4.0, z=5.0)
  fly2 = PhysicalCoord(x=9.0, y=10.0, z=11.0)
  print(f"physical position of fly 1 ü™∞ is {fly1}")
  print(f"physical position of fly 2 ü™∞ is {fly2}")
```

Although the example is trivial, it illustrates the *essence* in representing data *continuously*.
Rather than discretely enumerate through all possible positions of a plane,
the type is compressed and uniquely described by three fields of `x`, `y` and `z`
so that any position on a plane
‚Äî or at least up until the maximum value that Python's floating point numbers support ‚Äî
can be described by three numbers.

The three fields with floating point values which the `PhysicalCoord3D` type consists of
can be used to model phenomena too besides that of physical space.
For instance, consider modeling houses, where each house object is a coordinate in some logical space of homes.
This is done by replacing the three earlier fields of `length`, `width` and `height`
that that of `size`, `age`, and `rooms`:

```python
from dataclasses import dataclass

@dataclass
class HouseCoord:
  size:  float # ft^2
  age:   float # yrs
  rooms: float # count

  def __repr__(self) -> str:
    return f"({self.size}, {self.age}, {self.rooms})"

if __name__ == "__main__":
  house1 = HouseCoord(size=3.0, age=4.0, rooms=5.0)
  house2 = HouseCoord(size=9.0, age=10.0, rooms=11.0)
  print(f"logical position of house 1 is {house1}")
  print(f"logical position of house 2 is {house2}")
```

And keep in mind that `HouseCoord3D` is not a coordinate in physical space,
but rather one in a logical space.
Moving on, there's nothing stopping us from increasing the expressivity of the house type
adding fourth field whose value is the number of floors.

```python
from dataclasses import dataclass

@dataclass
class HouseCoord:
  size:   float # ft^2
  age:    float # yrs
  rooms:  float # count
  floors: float # count

  def __repr__(self) -> str:
    return f"({self.size}, {self.age}, {self.rooms}, {self.floors})"

if __name__ == "__main__":
  house1 = HouseCoord(size=3.0, age=4.0, rooms=5.0, floors=2.0)
  house2 = HouseCoord(size=9.0, age=10.0, rooms=11.0, floors=3.0)
  print(f"logical position of house 1 is {house1}")
  print(f"logical position of house 2 is {house2}")
```

and so on, up until some arbitary $n$ number of fields.
Taking a step back, what `PhysicalCoord`, `HouseCoord`
share in common is that they are *lists of numbers*, indexed by interpretable fields.
To generalize and abstract these escape hatches that allow us to represent types continuously,
we can define a generic `Coord4Fields` dataclass with fields labeled as `xi` for `0<=i<=4`,
and push the responsibility of keeping track of which field (dimension) corresponds to which attribute to the caller.

```python
from dataclasses import dataclass

@dataclass
class Coord4Fields:
  x0: float
  x1: float
  x2: float
  x3: float

  def __repr__(self) -> str:
    return f"({self.x0}, {self.x1}, {self.x2}, {self.x3})"

if __name__ == "__main__":
  house1 = Coord4Fields(x0=3.0, x1=4.0, x2=5.0, x3=2.0)
  house2 = Coord4Fields(x0=9.0, x1=10.0, x2=11.0, x3=3.0)
  print(f"logical position of house 1 is {house1}")
  print(f"logical position of house 2 is {house2}")
```

```bob


  0D            1D              2D                3D                      4D
 Point         Line           Square             Cube                 Tesseract

                                +-----+        +-------+             +-------+
                                |     |       /|      /|            /|+-----+|
                                |     |      / |     / |           / ||     ||
   *          *----*            |     |     +-------+  |          +-------+  |
                                |     |     |  +----|--+          |  |+-----|+
                                |     |     | /     | /           | /| |   /||
                                +-----+     |/      |/            |/ +-|---+ |
                                            +-------+             +---|---+| |
                                                                  |   +---|-|-+
                                                                  |  /|   | |/
                                                                  | / |   | +
                                                                  |/  |   |/
                                                                  +-------+

  Figure n
```
If we want a general data structure that works for any number of fields $n$,
we can simply reuse Python's builtin `list[]`, limiting it's elements to floating point numbers with `list[float]`.
When representing types continuously, these lists of numbers are referred to as **vectors**,
which is more precisely defined as a mapping from some $n\in\Z$ to $\reals$,
denoted by $\reals^n$, which is exactly the functionality that a `list[]` provides:

define notions of dependance, basis, and dimension copmutationally..

```python
if __name__ == "__main__":
  house1 = [3.0, 4.0, 5.0, 2.0]
  print(f"logical position of house 1 is ({house1[0]}, {house1[1]}, {house1[2]}, {house1[3]})")
```

With the generality of the `list[float]` data structure,
we can continue to add an arbitrary number of elements which capture other aspects to what makes a home a home.
But now consider writing a program that can *update* and *change* homes with renovations.
i.e given a house, perform a renovation that increases the square feet by 10.0,
no change to the age, two more rooms, and another floor.
Perhaps adding the two lists will evaluate the intended result?
```python
if __name__ == "__main__":
  house1 = [3.0, 4.0, 5.0, 2.0]
  renovation = [10.0, 0.0, 2.0, 1]
  house1_renovated = house1 + renovation
  print(f"logical position of house1 renovated {house1_renovated}")
```

The `+` operation defined on `list[]` is clearly non-sensible for our use case.

```python
if __name__ == "__main__":
  house1 = [3.0, 4.0, 5.0, 2.0]
  house1 = [3.0, 4.0, 5.0, 2.0]
  house1_renovated = house1 + house1
  print(f"house1 renovated {house1_renovated}")
```

produces another one that's  twice as big, 

develop a *linear arithmetic and algebra* for these lists of numbers x+y and a*x
why linear?

```python
if __name__ == "__main__":
  house1 = [3.0, 4.0, 5.0, 2.0]
  house2 = [9.0, 10.0, 11.0, 3.0]
  house3 = house1 + house2
  house4 = house1 * 3
  print(house3)
  print(house4)
  # print(f"position of house 1 is {house1[0]}, {house1[1]}, {house1[2]}, {house1[3]}")
```

computational perspective..list of numbers
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/fNk_zzaMoSs?si=RM4DkmjjxivKNjN7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

what is the mathematical perspective?

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/TgKwz5Ikpc8?si=IA44m9h0QALa-YKR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/k7RM-ot2NWY?si=GK0BOgsmchUV52Mx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 1.2.2 Linear Transformations with Matrix-Vector Multiplication
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/kYB8IZa5AuE?si=GPQlzJ1uEg83PX8o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


### 1.2.3 Predicting Scalars with Linear Regression and Mean Squared Error Loss
The primary goal of the statistical learning approach to artificial intelligence
is *induction*, which is to use data in order to make accurate predictions in new situations
with some function $f: \mathcal{X} \to \mathcal{Y}$.
When there's access to historical data of the expected output $y$ for some input $x$, we refer
to this regime as *supervised learning*, for the learning algorithm is being "supervised" with the right answer.

In particular, the dataset is split into a *training set* $d_{\text{train}}= \{(x^{(i)},y^{(i)})\}_{i=0}^n$,
and a *testing set* $d_{\text{test}}= \{(x^{(i)},y^{(i)})\}_{i=0}^{n^*}$.
Once the function $f$ is recovered from the training set, it's ability to generalize is evaluated on the test set.
that is, predict a value $y^{(n+1)}$ given some unseen observation $x^{(n+1)}$.
Machine learning has its roots in statistical inference, and so we also refer to
the data as observations or evidence, and the function as hypotheses or models.

For instance, consider the task of *house price prediction*[^5] where the collected [dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
$d= \{(x^{(i)},y^{(i)})\}_{i=0}^n$ are input-output pairs of square feet, and dollars, respectively.
Let's take a look at this data.

```python
import matplotlib.pyplot as plt
X, Y = [1500, 2100, 800], [500000, 800000, 250000]
plt.scatter(X,Y)
plt.title("Price vs Size")
plt.xlabel('ft^2')
plt.ylabel('$')
plt.show()
```

<div id="pyodide-plot"></div>
<script src="https://cdn.jsdelivr.net/pyodide/v0.25.1/full/pyodide.js"></script>
<script>
(async () => {
  const pyodide = await loadPyodide({ indexURL: "https://cdn.jsdelivr.net/pyodide/v0.25.1/full/" });
  await pyodide.loadPackage("micropip");
  const micropip = pyodide.pyimport("micropip");
  await micropip.install("matplotlib");
  await pyodide.runPythonAsync(`
import io, base64
import matplotlib.pyplot as plt
from js import document
fig, ax = plt.subplots(figsize=(3, 2.25))
X, Y = [1500, 2100, 800], [500000, 800000, 250000]
plt.scatter(X,Y)
plt.title("Price vs Size")
plt.xlabel('ft^2')
plt.ylabel('$')
buf = io.BytesIO()
fig.savefig(buf, format="png", dpi=100, bbox_inches="tight")
buf.seek(0)
img = document.createElement("img")
img.src = "data:image/png;base64," + base64.b64encode(buf.read()).decode("ascii")
document.getElementById("pyodide-plot").appendChild(img)
plt.close(fig)
`);
})();
</script>

In the case of house price prediction,
the input space $\mathcal{X}=\reals$ is the size in square feet, and the output space $\mathcal{Y}=\reals$ is the price,
which means the function to recover has type $f:\reals \to \reals$.
Since enumerating through every house size and price pair in $\reals$ is intractable
(otherwise all pairs could be inserted inside a lookup), an assumption
about the structure of the data *needs* to be made, referred to as the
*inductive bias*.
The simplest assumption to make is to *assume* that there exists a linear relationship
between the data so that $f$ is determined by two parameters of slope $m$ and intercept $b$. That is,
$$
\begin{aligned}
f &:\reals \to \reals\\
f(x;m,b) &\coloneqq \underbrace{m}_{\text{parameter}}x+\underbrace{b}_{\text{parameter}}
\end{aligned}
$$

where $f(;)$ is used to denote the differentiation between input and parameter[^6].
Implementing computable $f$ looks like the following,
where each evaluation of `fhat()` passes in a random number for the slope parameter of `x`.
Feel free to modify the range for $m$ and run the code a few times to see the different values `fhat()` produces as predictions.

```python
def fhat(x: float, m: float, b: float) -> float:
  yhat = x*m+b
  return yhat

if __name__ == "__main__":
  import random
  X, Y = [1500, 2100, 800], [500000, 800000, 250000]
  m, b = random.uniform(-3.0, 3.0), random.uniform(-3.0, 3.0)

  for (xi,yi) in zip(X,Y):
    yihat = fhat(xi,m,b)
    print(f"expected: ${yi}, actual: ${yihat:.2f}")
```

And instead of evaluating `fhat()` for each input `xi` sequentially in a loop,
we can change the type of the input from a *scalar* $x \in \reals$ to a *vector* $X \in \reals^n$
so that the predictions can be evaluated in a single vector-scalar multiplication
(and subsequently vector-scalar addition) where
$$
\begin{aligned}
f_{\text{batched}} &:\reals^n \to \reals^n\\
f_{\text{batched}}(\mathbf{x};m,b) &\coloneqq m\mathbf{x}+b
\end{aligned}
$$

Some languages are adding support for multidimensional arrays in their standard libraries (todo, footenote with cpp),
but all numerical computing in python which involve the evaluation of
high dimensional functions are conducted in the `numpy` package which provides the `numpy.ndarray` data structure
and operations *accelerated* by vector instructions in hardware.

```python
import numpy as np

def fhatbatched(X_n: np.ndarray, m: float, b: float) -> np.ndarray:
  print(f"X_n.shape:{X_n.shape}")
  yhat_n = X_n*m+b # vector-scalar multiply and add uses vectorized hardware instructions
  return yhat_n

if __name__ == "__main__":
  import random
  X, Y = np.array([1500, 2100, 800]), [500000, 800000, 250000]
  m, b = random.uniform(-3.0, 3.0), random.uniform(-3.0, 3.0)

  yhats = fhatbatched(X,m,b) # evaluating fhat on all inputs in one function call
  for y, yhat in zip(Y, yhats):
    print(f"expected: ${y}, actual: ${yhat:.2f}")
```

Because the two code snippets seem to have run in relatively the same amount of time,
a reasonable question at this point is to ask
how *much* faster is the batched implementation with hardware accelerated vector instructions
compared to the vanilla-python sequential implementation?

The answer becomes apparent when the size of vectors $v \in \reals^n$ is increased.
For instance, when the number of house data in our dataset increases to $1e3$, $1e6$, $1e9$,
the difference in wallclock time between the two implementations becomes more drastic.
Run the code below which updates `n` from `3` to `10,000,000` (which will take a few seconds)
in order to measure the difference in wallclock time.
(Note that the construction of dataset `Y` is omitted to simply compare the evaluation of the sequential and vectorized implementations of $f$.)

```python
import numpy as np

def fhat(x: float, m: float, b: float) -> float: return x*m+b
def fhatbatched(X_n: np.ndarray, m: float, b: float) -> np.ndarray: return X_n*m+b

if __name__ == "__main__":
  import random, timeit
  X = np.random.rand(10_000_000)
  m, b = random.uniform(-3.0, 3.0), random.uniform(-3.0, 3.0)

  def sequential(): return [fhat(x,m,b) for x in X]
  def vectorized(): return fhatbatched(X,m,b)
  scalar_time, vector_time = timeit.timeit(sequential, number=5), timeit.timeit(vectorized, number=5)

  print(f"sequential time: {scalar_time:.2f} sec")
  print(f"vectorized time: {vector_time:.2f} sec")
```

Analyzing the two times printed to stdout, the majority if not all of the execution time
was spent waiting for the sequential implementation to complete.
This is because `np.ndarray`'s implementations of `*` and `+` are accelerated using
*vectorized hardware instructions*, which we will implement for ourselves in sections
[1.3 Programming High Dimensional Data and Functions](#12-programming-high-dimensional-data-and-functions) and
[1.4 Accelerating High Dimensional Functions with the `BLAS`](#13-accelerating-blas-kernels-on-latency-oriented-multi-core-processors).
For now, we will continue to use the `np.ndarray` to carry out the task of price prediction
(even though the specific housing dataset we are working with only has 516 entries.)

**1.2.2 Fitting a Line in Higher Dimensions**
Before we recover the parameters $m$ and $b$ which characterize the function $\hat{f}$
that best fits the data, let's increase the expressivity of our function in order to model
price not simply as a relation of size but also of the other variables available from the dataset.
This means modifying the domain of our function from $\reals$ to $\reals^m$
so that house price is modeled as a function of form

$$
\begin{aligned}
f &:\reals^m \to \reals\\
f(\mathbf{x};\theta) &\coloneqq \theta^{\top}\mathbf{x}\\
                     &= \sum_{i=0}^{n} \theta_i\mathbf{x}_i
\end{aligned}
$$

where $\theta \in \reals^m$ is the vector of parameters and $\mathbf{x}_0 = 1$ so that $\theta_0$ is the bias.
Note that we are now using $m$ to denote the size of the feature space rather than slope
so that $f$ accepts not a scalar $x$ but a vector $\mathbf{x}$, with the codomain remaining $\reals$. So for instance,
$\mathbf{x}_0$ is size, $\mathbf{x}_1$ is age, $\mathbf{x}_2$ is the number of bedrooms,
$\mathbf{x}_{m-1}$ is the location, each weighted by a parameter $\theta_i$
indicating how important that feature is relative to the final price.
We can still define the *batched* evaluation over the entire dataset $d$ with

$$
\begin{aligned}
f_{\text{batched}} &:\reals^{n\times m} \to \reals^n\\
f_{\text{batched}}(\mathbf{X};\theta) &\coloneqq \mathbf{X}\theta\\
\end{aligned}
$$

where $\mathbf{X}$ is a *data/design* matrix which stores $n$ vectors in $\reals^m$.
In Python, this looks like the following:

```python
import numpy as np

def fhatbatched(X_n: np.ndarray, theta: np.ndarray) -> np.ndarray:
  print(f"X_n.shape:{X_n.shape}")
  yhat_n = X_n@theta # vector-scalar multiply and add uses vectorized hardware instructions
  return yhat_n

if __name__ == "__main__":
  import random
  X, Y = np.array([[1500], [2100], [800]]), [500000, 800000, 250000]  # X is (n, m) where n=3, m=1
  theta = np.random.uniform(-3.0, 3.0, size=(1,))  # theta is shape (m,)

  yhats = fhatbatched(X,theta) # evaluating fhat on all inputs in one function call
  for y, yhat in zip(Y, yhats):
    print(f"expected: ${y}, actual: ${yhat:.2f}")
```

### 1.2.4 Maximizing Likelihood Implies Mean Squared Error
After the inductive bias on the family of functions has been made, the
learning algorithm must find the function $\hat{f}$ with a good fit.
Since artificial learning algorithms don't have visual cortex like biological humans[],
the notion of "good fit" needs to defined in a systematic fashion.
This is done by selecting the parameter $\theta \in \reals^m$ which *maximizes the likelihood of the data* $p(d;\theta)$.
Returning to the linear regression inductive bias we've selected to model the house price data,
we *assume* there exists *noise* $\epsilon^{(i)}$
in both our *model* (epistemic uncertainty) and *data* (aleatoric uncertainty),
so that $y^{(i)} = \theta^{\top}\mathbf{x}^{(i)} + \epsilon^{i}$ where $\epsilon^{(i)} \sim \mathcal{N(\mu, \sigma^2)}$

prices $y^{i}$ are *normally distributed conditioned* on seeing the features $\mathbf{x}^{i}$
with the *mean* being the equation of the line $\theta^{\top}\mathbf{x}^{(i)}$
where $y^{(i)}|\mathbf{x}^{(i)} \sim \mathcal{N}(\mu=\theta^{\top}\mathbf{x}^{(i)}, \sigma^2)$,
then we have that

$$
\begin{aligned}
p(y^{(i)}|\mathbf{x}^{(i)};\theta) &= \mathcal{N}(\theta^{\top}\mathbf{x}^{(i)}, \sigma^2) \\
                                  &= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y^{(i)}-\theta^{\top}\mathbf{x}^{(i)})^2}{2\sigma^2}\right)
\end{aligned}
$$

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/q7seckj1hwM?si=YYH1Z3zRbx-K0AF-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Returning to the linear regression model, we can solve this optimization with a *direct method* using normal equations.
QR factorization, or SVD.

```python
def fhatbatched(X_n: np.ndarray, m: float, b: float) -> np.ndarray: return X_n*m+b

if __name__ == "__main__":
  X, Y = np.array([1500, 2100, 800]), np.array([500000, 800000, 250000]) #  data

  X_b = np.column_stack((np.ones_like(X), X))              # [1, x]
  bhat, mhat = np.linalg.solve(X_b.T @ X_b, X_b.T @ Y)   # w = [b, m]

  yhats = fhatbatched(X, mhat, bhat) # yhat
  for y, yhat in zip(Y, yhats):
    print(f"expected: ${y:.0f}, actual: ${yhat:.2f}")
```

To summarize, we have selected and computed
1. an inductive bias with the family of linear functions $f(\mathbf{X};\theta)\coloneqq \mathbf{X}@\theta$
2. an inductive principle with the least squared loss $\mathscr{L}(\theta) \coloneqq \sum_{i=0}^{n} (y^{(i)}-\mathbf{X}@\theta)^2$
3. the parameters which minimze the empirical risk, denoted as $(\hat{\theta}) = \argmin \mathscr{L}(\theta)$

Together,
the inductive bias describes the relationship between the input and output spaces,
the inductive principle is the loss function that measures prediction accuracy,
and the minimization of the empirical risk finds the parameters for the best predictor.


### 1.2.5 Predicting Categories with Logistic Regression and Cross Entropy Loss

### 1.2.6 Fitting a Line Iteratively with Derivatives of Hilbert Space in Gradient Descent

### 1.2.7 Generalized Linear Models: Linear and Logistic Regression

### 1.2.8 Generalization, Bias-Variance Tradeoff, and Double Descent

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/z64a7USuGX0?si=4r_fhjr59_DDjwiO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/Ip3X9LOh2dk?si=hIN4Lz7S0H6qR5z3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/uQhTuRlWMxw?si=Jc-vZaoM-GlCm-XI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<!-- Armed with an operational understanding of how software 2.0 employs the power of the multidimensional array with statistical learning,
we will learn about the internals and implementation details of the `numpy` `nd.array`
in [1.4 Programming `teenygrad` *Level -1*: Tensor, the Multidimensional Array]()
by programming our very own multidimensional array with strides which virtualizes<span class="sidenote-number"></span><span class="sidenote">*Virtualizing physical resources into logical ones can be seen in the languages and runtimes of software 1.0. i.e. C++'s `std::iterator` virtualizing physical containers, operating system memory virtualizing physical pages, etc.*</span> a physical 1D buffer into arbitrary ND logical arrays.
This multidimensional array we'll define as `teenygrad.Tensor`
serves as the first piece of the deep learning framework `teenygrad`, which is the capstone project developed throughout the book. -->

linearity depends on that the scalars are in a field (addition is associative)
"scalars" in a field --> we are dealing with floating point representations
"vectors" we saw coordinate systems in 1.1.4
"matrices" and we saw linear transforms in 1.2.1
mathematically, a vector is a coordinate in some basis of a vector space
                a matrix is a linear function,
the linear, in numerical linear algebra, is aspirational.

## 1.3 Programming CPU Accelerated Basic Linear Algebra Subroutines with `teenygrad.Tensor`
<small>[$\hookleftarrow$ Table of Contents](#table-of-contents)</small>

### 1.3.1 The Simplest `Tensor` Implementation with Nested `array`s
Now that we have an operational understanding of how multidimensional arrays are used in practice,
let's now implement our very own `np.array`, which will serve as the foundation to 
the book's capstone project ‚Äî the deep learning framework `teenygrad`.

Let's start with the simplest implementation idea for `teenygrad.Tensor`,
which is a nested array implementation with Python's built-in [`list`](https://docs.python.org/3/library/stdtypes.html#lists) data structure.

For some $x \in \reals$ we will use a `float`,
for $\mathbf{x} \in \reals^{n}$, we will use a `list[float]`,
and for arbitary $\mathbf{x} \in \reals^{d_0 \times \cdots \times d_n}$ we will use a `list[list[float]]`.
In fact, this is simply what we started doing in the previous chapter when implementing our linear regression model before switching to `numpy`'s accelerated `nd.array`.

### 1.3.2 Virtualizing ND Logical Shapes to 1D Physical Storages with Strides
```python
class Tensor():
  def __init__():
    self.shape: tuple[int] | None = []
    self.stride: tuple[int] | None = []
    self.buf: None = None
    self.offset: int = 0
```

<!-- Decoupling Views and Copies... -->

### 1.3.3 Implementing Basic Linear Algebra Subroutines in Python: `AXPY`, `GEMV` and `GEMM`

```python
class Tensor():
  # ...
  def 
```

### 1.3.4 Accelerating the `BLAS` with Processor Pipelines and Memory Hierarchies


```rust
/// SGEMM with the classic BLAS signature (row-major, no transposes):
/// C = alpha * A * B + beta * C
fn sgemm(
  m: usize, n: usize, k: usize,
  alpha: f32, a: &[f32], lda: usize,
  b: &[f32], ldb: usize,
  beta: f32, c: &mut [f32], ldc: usize) {
  assert!(m > 0 && n > 0 && k > 0, "mat dims must be non-zero");
  assert!(lda >= k && a.len() >= m * lda);
  assert!(ldb >= n && b.len() >= k * ldb);
  assert!(ldc >= n && c.len() >= m * ldc);

  for i in 0..m {
    for j in 0..n {
      let mut acc = 0.0f32;
      for p in 0..k { acc += a[i * lda + p] * b[p * ldb + j]; }
      let idx = i * ldc + j;
      c[idx] = alpha * acc + beta * c[idx];
    }
  }
}

fn main() {
  use std::time::Instant;

  for &n in &[16usize, 32, 64, 128, 256] {
    let (m, k) = (n, n);
    let (a, b, mut c) = (vec![1.0f32; m * k], vec![1.0f32; k * n], vec![0.0f32; m * n]);

    let t0 = Instant::now();
    sgemm(m, n, k, 1.0, &a, k, &b, n, 0.0, &mut c, n);
    let secs = t0.elapsed().as_secs_f64().max(std::f64::MIN_POSITIVE);
    let gflop = 2.0 * (m as f64) * (n as f64) * (k as f64) / 1e9;
    let gflops = gflop / secs;

    println!("m=n=k={n:4} | {:7.3} ms | {:6.2} GFLOP/s", secs * 1e3, gflops);
  }
}
```

<div class="full-bleed">
<iframe src="./assets/rooflinejupiter.html" width="100%" height="900" style="border:none; border-radius:12px;" loading="lazy"></iframe>
</div>
<img src="./assets/zen5gnrcore.webp" onmouseover="this.src='./assets/zen5gnrcoreannotated.webp'" onmouseout="this.src='./assets/zen5gnrcore.webp'"></br>
<small>*AMD Zen5 Core Die Shot by Fritzchens Fritz (Hover for annotation by Nemez)*</small>

Thus far in our journey we've successfully made the transition
from programming software 1.0 which involves specifying of discrete data structures ‚Äî like sets, associations, lists, trees, graphs, ‚Äî with the determinism of types
to programming software 2.0 which involves reovering continous data structures ‚Äî like vectors, matrices, and tensors ‚Äî with the stochasticity of probabilities.
If you showed the mathematical equations for your models, loss functions, and optimizers to our mathematical cousins<span class="sidenote-number"></span><span class="sidenote">*In particular, the physicists whom realized describing reality as minimizing free-energy was fruitful, such as [Helmholtz with energy](https://en.wikipedia.org/wiki/Helmholtz_free_energy), [Gibbs with enthalpy](https://en.wikipedia.org/wiki/Gibbs_free_energy), and [Boltzmann with entropy](https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula).*</span> who also made the same transition,
they would understand the mathematical equations and even algorithms<span class="sidenote-number"></span><span class="sidenote">*as until recently, algorithms were understood to be a sequence of instructions to be carried out by a human. i.e [Egyptian Multiplication](https://en.wikipedia.org/wiki/Ancient_Egyptian_multiplication), [Euclid's Greatest Common Divisor](https://en.wikipedia.org/wiki/Euclidean_algorithm)*</span>.
But they wouldn't understand how the code we've programmed thus far with Python and Numpy is being automatically calculated by the mechanical machine we call computers.
For that we need to transition from *users* of the `numpy` framework to implementors our own framework, which we'll call `teenygrad`.
This requires us moving from the beautiful abstraction of mathematics to the assembly heart and soul of our machines.
This book uses Rust<span class="sidenote-number"></span><span class="sidenote">*To backtrack to familiarize yourself with Rust, take a look at the experimental version of [The Rust Programming Language](https://rust-book.cs.brown.edu/), by from Will Crichton and Shriram Krishnamurthi, originally written by Steve Klabnik, Carol Nichols, and Chris Krycho.*</span>, but feel free to follow along with C/C++<span class="sidenote-number"></span><span class="sidenote">*In that case, take a look at "The C Programming Language by Brian Kernighan and Dennis Ritchie.*</span> ‚Äî what's fundamental to the purpose of accelerating said basic linear algebra subroutines on the CPU is
is choosing a language that 1. forgoes the dynamic memory management overhead of garbage collection and
2. has a compiled implementation which allows us to analyze and tune the actual instructions which an underlying processor executes.
Using Rust will allow us to start uncovering what is really happening under the hood of accelerated Tensors
<span class="sidenote-number"></span><span class="sidenote">*To sidetrack to familiarize yourself with systems programming in the context of software 1.0 , read the classic of [Computer Systems A Programmer's Perspective](https://csapp.cs.cmu.edu/)*</span>with instruction set architectures, microarchitecture, memory hierarchies

- instruction set architecture
- computation: control path, data path
- communication: memory, input/output

In the last chapter we developed `teenygrad.Tensor` by virtualizing physical 1D storage buffers into
arbitrarily-shaped logical ND arrays by specifying an iteration space with strides,
and started our journey in accelerating the basic linear algebra subroutines defined on the `Tensor` with Rust,
which speeds up the throughput performance of SGEMM from `10MFLOP/S` to `1GFLOP/S`.
Simply executing a processor's native code ‚Äî referred to as assembly code ‚Äî rather than Python's bytecode
results in an increase of two orders of magnitude.

Now that the code that is being executed is native assembly,
we can dive deeper into the architecture of the machine in order to reason and improve
the performance of `teenygrad`'s `BLAS`.

<!-- <img src="./assets/zen5gnrccd.jpg" onmouseover="this.src='./assets/zen5gnrccdannotated.jpg'" onmouseout="this.src='./assets/zen5gnrccd.jpg'"> -->

<div class="full-bleed">
<iframe height="1000px" src="https://godbolt.org/e#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,selection:(endColumn:2,endLineNumber:19,positionColumn:2,positionLineNumber:19,selectionStartColumn:2,selectionStartLineNumber:19,startColumn:2,startLineNumber:19),source:'pub+fn+gemv(%0A++++m:+usize,+n:+usize,%0A++++alpha:+f32,+beta:+f32,%0A++++a:+%26%5Bf32%5D,+x:+%26%5Bf32%5D,+y:+%26mut+%5Bf32%5D,%0A)+%7B%0A++++let+mut+i+%3D+0usize%3B%0A++++while+i+%3C+m+%7B%0A++++++++let+mut+sum+%3D+0.0f32%3B%0A%0A++++++++let+mut+j+%3D+0usize%3B%0A++++++++while+j+%3C+n+%7B%0A++++++++++++sum+%2B%3D+a%5Bi+*+n+%2B+j%5D+*+x%5Bj%5D%3B%0A++++++++++++j+%2B%3D+1%3B%0A++++++++%7D%0A%0A++++++++y%5Bi%5D+%3D+alpha+*+sum+%2B+beta+*+y%5Bi%5D%3B%0A++++++++i+%2B%3D+1%3B%0A++++%7D%0A%7D'),l:'5',n:'0',o:'Rust+source+%231',t:'0')),k:44.994347791517754,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:r1920,filters:(b:'0',binary:'1',binaryObject:'1',commentOnly:'0',debugCalls:'1',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'0',trim:'1',verboseDemangling:'0'),flagsViewOpen:'1',fontScale:14,fontUsePx:'0',j:1,lang:rust,libs:!(),options:'--target%3Driscv64gc-unknown-linux-gnu+-C+opt-level%3D1+-C+overflow-checks%3Doff+-C+panic%3Dabort',overrides:!((name:edition,value:'2024')),selection:(endColumn:1,endLineNumber:1,positionColumn:1,positionLineNumber:1,selectionStartColumn:1,selectionStartLineNumber:1,startColumn:1,startLineNumber:1),source:1),l:'5',n:'0',o:'+rustc+1.92.0+(Editor+%231)',t:'0')),k:55.00565220848227,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4"></iframe></div>

### 1.3.5 Accelerating Computation with *Instruction Level Parallelism* via Loop Unrolling
### 1.3.6 Accelerating Computation with *Data Level Parallelism* via Vector and Matrix Extensions
### 1.3.7 Accelerating Communication with *Memory Hierarchies* via Register and Cache Blocking
### 1.3.8 Accelerating Computation with *Thread Level Parallelism*

---

[^0]: *For good reason, as they were very uncomfortable with the incommensurability of the square root of 2*
[^1]: *As opposed to other formalisms such as probabilistic logic or uncertainty quantification.*
[^2]: *Axioms can be tested in the same way tests for programs can be tested ‚Äî do they capture the intended model? The first two axioms are quite easy to accept, for two numbers are needed to represent impossible and guaranteed events. The third axiom...(todo)*
[^3]: *Interestingly enough, a notion in algorithmic information theory formalized by the name [Solomonoff-Kolmogorov-Chaitin Complexity](https://en.wikipedia.org/wiki/Kolmogorov_complexity), for Kolmogorov first published on the subject in 1963.*
[^4]: *Or really, according to what *kind* of phenomena we want *a* definition to accurately model.*
[^5]: *While predicting house prices is not AGI, we follow the canonical hello world of machine learnign in order to provide an easy example while introducing the foundational mechanics of statistical learning, which are crucial to understanding part two and part three of the book where we cover more industrial deep learning. In software 1.0 you should know how to reverse a linked list. In software 2.0 you should know how to fit a line.*
[^6]: *If you are familiar with functional programming, this is the mathematical equivalent of currying via closure*.