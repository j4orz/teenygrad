![](./assets/adam.jpg)
<small>*The Creation of Adam, Michelangelo 1508-1512.*</small></br>

# II. Neural Networks

In part one of the book you have developed a
solid foundation to the statistical learning approach towards artificial intelligence
— it's amazing how close you are to that of deep learning.
By now, you understand that fuzzy phenomena such as that of house prices or language sentiment
are better described with *continuous stochastic* descriptions.
1. The funtions of software 2.0 are *continuous* in nature because
data is represented with higher dimensional coordinates rather than some discrete type
so the functions for the case of regression and classification
are of the form $f:\reals^d \to \reals$ and $f:\reals^d \to [0,1]$.
2. They are also *stochastic* in nature because the function bodies
are assigned conditional distributions $f(\mathbf{x}):=p_{\theta}(Y=y|X=x)$.
That is, instead of *declaring* some deterministic function computable Turing Machine
with control flow and assignment, the functions are *recovered* by
parameter estimation of $\theta$ via iterative optimization.

In part two of the book, you will train deep neural networks with `pytorch`,
which has the same multidimensional array as `numpy`, but in addition,
provides 1. neural network primitives, 2. automatic differentiation, and 3. gpu acceleration.
Because you have the necessary statistical learning foundation,
we will simply train one network architecture after another,
following the *2012-2020* time period of what is colloquially known as the *age of research*,
as research with different and novel inductive biases.
After training your first deep neural networks with `pytorch`,
you will update the implementation of `teenygrad` with those 3 features,
and train the very same nets.

With a solid understanding of how statistical learning recovers stochastic maps of data in higher dimensional coordinate spaces,
you are now ready to start training your first deep neural networks. The subdiscipline of training deep neural nets follows the same
practice of given some dataset $d=\{(x^{(i)},y^{(i)})\}_{i=1}^n$, declaring your model's forward pass $\hat{y}=f(\mathbf{x};\theta)$, it's scoring function $\ell(y,\hat{y}, \theta)$
and resulting backward pass $\ell'(y,\hat{y},\theta)$, and iteratively optimizing such score with gradient descent,
where $\theta^{t+1} := \theta^{t} - \eta\nabla\mathscr{L}(d, \theta)$.
The only difference as we'll shortly see soon is that the inductive bias used to model the function $f$ includes *non-linearities* $\varphi$ with many layers of *composition*.
This second part of the book focuses on the unsupervised/self-supervised
task of *sequence learning* building up to the transformer architecture behind GPT2,
but let's train our first nets using the same tasks from part one to ensure we understand the core foundation of deep neural nets.

After part two, you will be ready for part three of the book,
which follows the *2020-2025* time period of what is coloquially known amongst researchers as the *age of scaling*.
This time period focused on scaling up the generality of the attention mechanism in the transformers network architecture
with more data and compute following scaling laws, in which the software and hardware like pytorch and GPUs evolved to support.
In part three, you will add midtraining and posttraining phases to your pretrained transformer to add assistant and reasoning behavior,
and evolve `teenygrad`'s implementation for the final time to support a graph mode fusion compiler, and inference engine.


## Table of Contents

- [2.1 Sequence Learning with Deep Neural Networks in `pytorch`](#21-learning-sequences-with-deep-neural-networks-in-torch)
  - [2.1.1 Learning Non-Linear Representations with Feedforward Neural Networks]()
  - [2.1.2 Learning Embeddings with Feedforward Neural Networks]()
  - [2.1.3 Learning Sequences with Feedforward Neural Networks]()
  - [2.1.4 Learning Sequences with Convolutional Neural Networks]()
  - [2.1.5 Learning Sequences with Recurrent Neural Networks]()
  - [2.1.6 Learning Sequences with Bidirectional Encoder Representations from Transformers]()
  - [2.1.7 Learning Sequences with Generative Pretrained Transformers]()
- [2.2 Programming GPU Accelerated Forward and Backward Passes with `teenygrad.Tensor.backward()`](#22-programming-gpu-accelerated-automatic-differentation-with-teenygradtensorbackward)
  - [2.2.1 Symbolic, Numerical, and Automatic Differentiation]()
  - [2.2.2 Forward Mode Automatic Differentiation with `Tensor.forward()`]()
  - [2.2.3 Reverse Mode Automatic Differentiation with `Tensor.backward()`]()
  - [2.2.4 Iterative Optimization with Stochastic Gradient Descent `optim.sgd`]()
  - [2.2.5 Accelerating `GEMM` with Massively Parallel Processors]()
  - [2.2.6 Accelerating `GEMM` on GPU with Data Reuse]()
  - [2.2.7 Accelerating `GEMM` on GPU with Scheduling]()
  - [2.2.8 Accelerating `GEMM` on GPU with Tensor Cores]()

## 2.1 Learning Sequences with Deep Neural Networks in `torch`
<small>[$\hookleftarrow$ Table of Contents](#table-of-contents)</small>

### 2.1.1 Learning Non-Linear Representations with Feedforward Neural Networks

Recall that the task of house price prediction and sentiment classification which can be modelled
by functions of the form $f:\reals^d \to \reals$ and $f:\reals^d \to [0,1]$ respectively.
The simplest inductive bias was made, in a which a linear relationship was assumed to hold between the input and output spaces,
and where the output was subsequently modeled as an inner product $<,>: \reals^d \times \reals^d \to \reals$
between an input vector and a weight vector.
For the case of regression, we have $y=f(\mathbf{x};\theta):= \theta^{\top}\mathbf{x}$,
and for the case of classification, we have $y=f(\mathbf{x};\theta):= \sigma(\theta^{\top}\mathbf{x})$,
todo:glm,exp.
The key entry point into the function class of deep neural networks is that of logistic regression,
because the log odds produced by the inner product (which are indeed *affine*)
required a mapping into a valid probability via sigmoid function
$\sigma: \reals \to [0,1]$, where $\sigma(x):= \frac{1}{1+e(-x)}$ which is in fact not *linear* nor *affine*.

The next natural question to ask then is whether the logistic regression model is considered a deep neural network?
The answer is that technically *yes*, it can be considered a *degenerative* deep neural network with $0$ *hidden layers*<span class="sidenote-number"></span><span class="sidenote">*In the same way that a *list* can be considered a degenerative binary tree or graph*</span>.
These so-called hidden layers automate the construction of the *representation* through *learning*,
so that the model not only discovers the mapping from representation to output, but also the representation itself<span class="sidenote-number"></span><span class="sidenote">*In the same way that for certain computations the positional representation of arabic numerals are more suitable compared to that of roman numerals, and polar coordinates over cartesian coordinates*</span>.
The functions of deep neural networks will take the form of $f(\mathbf{x}):= g \circ \phi(\mathbf{x})$
with $g$ being a linear classifier on feature extractor
$\phi: \reals^{d_1} \to \reals^{d_n}$,
$\phi(\mathbf{x}):= h^{[l]} \circ h^{[l-1]} \circ \cdots h^{[1]} \circ h^{[0]}(\mathbf{x})$
where $l$ is the number of compositional *layers*, and each intermediate function $h^{[i]}$ has the form of $h^{[i]}: \reals^{d_i} \to \reals^{d_{i+1}}$.
Each hidden layer successively and gradually<span class="sidenote-number"></span><span class="sidenote">*In the same way of Grothendieck's preferred style of mathematics described in Récoltes et Semailles: I can illustrate the second approach with the same image of a nut to be opened. The first analogy that came to my mind is of immersing the nut in some softening liquid, and why not simply water? From time to time you rub so the liquid penetrates better, and otherwise you let time pass. The shell becomes more flexible through weeks and months—when the time is ripe, a touch of the hand is enough, and the shell opens like a perfectly ripened avocado! A different image came to me a few weeks ago. The unknown thing to be known appeared to me as some stretch of earth or hard marl, resisting penetration. One can go at it with pickaxes or crowbars or even jackhammers: this is the first approach, that of the “chisel” (with or without a hammer). The other is the sea. The sea advances insensibly and in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it… yet it finally surrounds the resistant substance.*</span> lifts the complexity and abstraction of the data's representation<span class="sidenote-number"></span><span class="sidenote">*Chris Olah, cofounder of Anthropic and the lead of it's interpretability research wrote an excellent article on how software 2.0's representation learning loosely correspond to software 1.0's types in [Neural Networks, Types, and Functional Programming](https://colah.github.io/posts/2015-09-NN-Types-FP/)*</span>.

**Together, these two aspects of learning *non-linear*, *representations* form the essence of *deep learning*.**

Let's now turn out attention to the function bodies of these $h^{[i]}$'s with a deep neural netork of $1$ hidden layer,
carrying out the task of price regression and sentiment classification so that $f$ has the form $f:\reals^d\to\reals$.
With the statistical learning foundation from part one, we will simply present the
forward pass $f(\mathbf{x})$, the loss function $\mathscr{L}(d, \theta)$, and the backward pass $\ell'(y,\hat{y},\theta)$.

*Forward Pass*

The functions of deep neural networks will take the form of $f(\mathbf{x}):= g \circ \phi(\mathbf{x})$
with $g$ being a linear classifier on feature extractor

$$
\begin{aligned}
\phi&: \reals^{d_1} \to \reals^{d_n} \\
\phi(\mathbf{x})&:= h^{[l]} \circ h^{[l-1]} \circ \cdots h^{[1]} \circ h^{[0]}(\mathbf{x})
\end{aligned}
$$
where $l$ is the number of compositional *layers*, and each intermediate function $h^{[i]}$ has the form of $h^{[i]}: \reals^{d_i} \to \reals^{d_{i+1}}$.

*Loss Function*

*Backward Pass*

An Interactive Node-Link Visualization of Convolutional Neural Networks
ISVC 2015 ADam W. Harley https://adamharley.com/nn_vis/harley_vis_isvc15.pdf
<div class="full-bleed">
<iframe height="750px" loading="lazy" src="https://adamharley.com/nn_vis/mlp/3d.html"></iframe>
</div>

<div class="full-bleed-side">
<iframe width="560" height="315" src="https://www.youtube.com/embed/aircAruvnKk?si=RarZ-IYLj4kWjxxl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/IHZwWFHWa-w?si=WqqxRg76vFBE6ww1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

- figure/diagram/lecun circuits ->algebraic/symbolic equations->torch code
  - first train net for price regression and classification
    - intuition of automating feature engineering
  - XOR: playground.tensorflow
  - change code below to XOR
```python
class MLP(nn.Module):
  """
  takes the previous block_size tokens, encodes them with a lookup table,
  concatenates the vectors and predicts the next token with an MLP.

  Reference:
  Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
  """

  def __init__(self, config):
    super().__init__()
    self.block_size = config.block_size
    self.vocab_size = config.vocab_size
    self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table
    # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token
    # before the beginning of the input sequence
    self.mlp = nn.Sequential(
      nn.Linear(self.block_size * config.n_embd, config.n_embd2),
      nn.Tanh(),
      nn.Linear(config.n_embd2, self.vocab_size)
    )

  def get_block_size(self):
    return self.block_size

  def forward(self, idx, targets=None):
    # gather the word embeddings of the previous 3 words
    embs = []
    for k in range(self.block_size):
      tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)
      idx = torch.roll(idx, 1, 1)
      idx[:, 0] = self.vocab_size # special <BLANK> token
      embs.append(tok_emb)

    # concat all of the embeddings together and pass through an MLP
    x = torch.cat(embs, -1) # (b, t, n_embd * block_size)
    logits = self.mlp(x)

    # if we are given some desired targets also calculate the loss
    loss = None
    if targets is not None:
      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

    return logits, loss
```

<div class="full-bleed">
<div style="overflow:hidden; height:660px; border:none;">
  <iframe src="https://playground.tensorflow.org"
    width="100%" height="860" scrolling="no"
    style="border:none; margin-top:-220px; overflow:hidden;"
    class="playground-iframe"></iframe>
</div>
</div>


<div class="full-bleed-side">
<iframe width="560" height="315" src="https://www.youtube.com/embed/l-9ALe3U-Fg?si=ifeERfdy_onDdlpj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="560" height="315" loading="lazy" src="https://www.youtube.com/embed/qx7hirqgfuU?si=EAPTizMLo2wDeesv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>





<!-- In part 1 of the book we trained generalized linear models of the form ___.
In part 2 we modify and increase the expressivity of the function class by including non-linearities $\varphi$.
The feedforward neural network simply put is a series of linear and non-linear layers
of the form $f:\reals^{d_0} \to \reals^{d_{l-1}}$ so
$$
f(\mathbf{x};w) := W^{(l-1)} \circ (\varphi \circ W^{(l-2)}) \circ \cdots \circ (\varphi \circ W^{(0)}) \circ \mathbf{x}
$$

where $W^{(i)} \in \reals^{d_{i-1}\times d_i}$, $\varphi:\reals^{d_i} \to \reals^{d_{i+1}}$ is an elementwise nonlinearity, and $w := (W^{(0)}, \dots , W^{(l-1)})$. Conceptually, the
linear layers are performing linear transformations that rotate, reflect, shear, and scale space, whereas the nonlinear transformations perform transformations that squash and twist space.

We will now use the same model of the feedforward neural network to accomplish two other goals.
Namely, representation learning, and sequence learning.

- representation learning
  - https://colah.github.io/posts/2015-09-NN-Types-FP/
  - https://arxiv.org/abs/1311.2901
- non-linear: playground.tensorflow -->

### 2.1.2 Learning Embeddings with Feedforward Neural Networks

<div class="full-bleed">
<iframe height="750px" loading="lazy" src="https://projector.tensorflow.org/" class="playground-iframe">></iframe>
</div>

### 2.1.3 Learning Sequences with Feedforward Neural Networks
<div class="full-bleed">
<iframe height="750px" loading="lazy" src="https://www.youtube.com/embed/LPZh9BOjkQs?si=qb8tsYdzxoG9pSpm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

<!-- A sequence model, simply put, is the conditional probability distribution of an output token given an input token $p(x^{(i)}|y^{(i)})$.
A sequence of tokens can be a sentence of words in the domain of language,
a series of pixels in the domain of vision, or a stream of waves in the domain of audio.

Since we are modeling language as stochastic phenomena, we use the formal language of probability theory,
where a probability space is a measurable space $(\Omega, \mathcal{F})$ with a measure $\mathbb{P}: \Omega \to \lbrack 0,1\rbrack$.
In the domain of language, the measurable space consists of a
sample space $\Omega$ which is the set of all *tokens* modelling a *vocabulary*,
and the event space $\mathcal{F}$ is the set of all *token combinations* which model a *language*.
The measure $\mathbb{P}: \mathcal{F} \to \lbrack 0,1\rbrack$
is the measure of the weight of a particular token combination (sentence, really) as an event
with respect to the set of all possible token combinations (sentences) as the entire event space.
Once we use a random variable $X$ to map events to $\reals$,
we can forget about the probability space and focus our attention on language models
which are joint probability distribution over all sequences of tokens. -->

<div class="full-bleed">
<iframe height="750px" loading="lazy" src="https://www.youtube.com/embed/TCH_1BHY58I?si=pyOwJBGyDinoif5i" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

```python
class MLP(nn.Module):
  """
  takes the previous block_size tokens, encodes them with a lookup table,
  concatenates the vectors and predicts the next token with an MLP.

  Reference:
  Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
  """

  def __init__(self, config):
    super().__init__()
    self.block_size = config.block_size
    self.vocab_size = config.vocab_size
    self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table
    # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token
    # before the beginning of the input sequence
    self.mlp = nn.Sequential(
      nn.Linear(self.block_size * config.n_embd, config.n_embd2),
      nn.Tanh(),
      nn.Linear(config.n_embd2, self.vocab_size)
    )

  def get_block_size(self):
    return self.block_size

  def forward(self, idx, targets=None):
    # gather the word embeddings of the previous 3 words
    embs = []
    for k in range(self.block_size):
      tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)
      idx = torch.roll(idx, 1, 1)
      idx[:, 0] = self.vocab_size # special <BLANK> token
      embs.append(tok_emb)

    # concat all of the embeddings together and pass through an MLP
    x = torch.cat(embs, -1) # (b, t, n_embd * block_size)
    logits = self.mlp(x)

    # if we are given some desired targets also calculate the loss
    loss = None
    if targets is not None:
      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

    return logits, loss
```

### 2.1.4 Learning Sequences with Convolutional Neural Networks
### 2.1.5 Learning Sequences with Recurrent Neural Networks
```python
class RNNCell(nn.Module):
  """
  the job of a 'Cell' is to:
  take input at current time step x_{t} and the hidden state at the
  previous time step h_{t-1} and return the resulting hidden state
  h_{t} at the current timestep
  """
  def __init__(self, config):
    super().__init__()
    self.xh_to_h = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)

  def forward(self, xt, hprev):
    xh = torch.cat([xt, hprev], dim=1)
    ht = F.tanh(self.xh_to_h(xh))
    return ht

class RNN(nn.Module):
  def __init__(self, config, cell_type):
    super().__init__()
    self.block_size = config.block_size
    self.vocab_size = config.vocab_size
    self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state
    self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table
    if cell_type == 'rnn':
        self.cell = RNNCell(config)
    elif cell_type == 'gru':
        self.cell = GRUCell(config)
    self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)

  def get_block_size(self):
    return self.block_size

  def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()

    # embed all the integers up front and all at once for efficiency
    emb = self.wte(idx) # (b, t, n_embd)

    # sequentially iterate over the inputs and update the RNN state each tick
    hprev = self.start.expand((b, -1)) # expand out the batch dimension
    hiddens = []
    for i in range(t):
      xt = emb[:, i, :] # (b, n_embd)
      ht = self.cell(xt, hprev) # (b, n_embd2)
      hprev = ht
      hiddens.append(ht)

    # decode the outputs
    hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)
    logits = self.lm_head(hidden)

    # if we are given some desired targets also calculate the loss
    loss = None
    if targets is not None:
      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

    return logits, loss
```

### 2.1.6 Learning Sequences with Generative Pretrained Transformers
```python
class NewGELU(nn.Module):
  """
  Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).
  Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415
  """
  def forward(self, x):
    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))

class CausalSelfAttention(nn.Module):
  """
  A vanilla multi-head masked self-attention layer with a projection at the end.
  It is possible to use torch.nn.MultiheadAttention here but I am including an
  explicit implementation here to show that there is nothing too scary here.
  """

  def __init__(self, config):
    super().__init__()
    assert config.n_embd % config.n_head == 0
    # key, query, value projections for all heads, but in a batch
    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
    # output projection
    self.c_proj = nn.Linear(config.n_embd, config.n_embd)
    # causal mask to ensure that attention is only applied to the left in the input sequence
    self.register_buffer("bias", torch.tril(torch.ones(config.block_size, config.block_size))
                                  .view(1, 1, config.block_size, config.block_size))
    self.n_head = config.n_head
    self.n_embd = config.n_embd

  def forward(self, x):
    B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)

    # calculate query, key, values for all heads in batch and move head forward to be the batch dim
    q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)

    # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)
    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
    att = F.softmax(att, dim=-1)
    y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side

    # output projection
    y = self.c_proj(y)
    return y

class Block(nn.Module):
  """ an unassuming Transformer block """

  def __init__(self, config):
    super().__init__()
    self.ln_1 = nn.LayerNorm(config.n_embd)
    self.attn = CausalSelfAttention(config)
    self.ln_2 = nn.LayerNorm(config.n_embd)
    self.mlp = nn.ModuleDict(dict(
        c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),
        c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),
        act     = NewGELU(),
    ))
    m = self.mlp
    self.mlpf = lambda x: m.c_proj(m.act(m.c_fc(x))) # MLP forward

  def forward(self, x):
    x = x + self.attn(self.ln_1(x))
    x = x + self.mlpf(self.ln_2(x))
    return x

class Transformer(nn.Module):
  """ Transformer Language Model, exactly as seen in GPT-2 """

  def __init__(self, config):
    super().__init__()
    self.block_size = config.block_size

    self.transformer = nn.ModuleDict(dict(
        wte = nn.Embedding(config.vocab_size, config.n_embd),
        wpe = nn.Embedding(config.block_size, config.n_embd),
        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),
        ln_f = nn.LayerNorm(config.n_embd),
    ))
    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

    # report number of parameters (note we don't count the decoder parameters in lm_head)
    n_params = sum(p.numel() for p in self.transformer.parameters())
    print("number of parameters: %.2fM" % (n_params/1e6,))

  def get_block_size(self):
    return self.block_size

  def forward(self, idx, targets=None):
    device = idx.device
    b, t = idx.size()
    assert t <= self.block_size, f"Cannot forward sequence of length {t}, block size is only {self.block_size}"
    pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)

    # forward the GPT model itself
    tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)
    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)
    x = tok_emb + pos_emb
    for block in self.transformer.h:
        x = block(x)
    x = self.transformer.ln_f(x)
    logits = self.lm_head(x)

    # if we are given some desired targets also calculate the loss
    loss = None
    if targets is not None:
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)

    return logits, loss
```

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/wjZofJX0v4M?si=QWmMr-44qolbqpvh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/eMlx5fFNoYc?si=5dr2YFW3KMSl1WRF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/9-Jl0dxWQs8?si=-XzJJ3h4Dp4TqVD4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/kCc8FmEb1nY?si=GtHe8YolWoz0sEHR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/D8GOeCFFby4?si=VW2DHR4JfSBNgYgx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## 2.2 Programming GPU Accelerated Automatic Differentation with `teenygrad.Tensor.backward()`
<small>[$\hookleftarrow$ Table of Contents](#table-of-contents)</small>

### 2.2.1 Symbolic, Numerical, and Automatic Differentiation
### 2.2.2 Forward Mode Automatic Differentiation with `Tensor.forward()`
### 2.2.3 Reverse Mode Automatic Differentiation with `Tensor.backward()`
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/VkHfRKewkWw?si=ctIS6a1CxJrUBCxI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/YG15m2VwSjA?si=z6eEC7e_4qDckMeB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
Consider the function $f: \reals, \reals \to \reals$ where $f \colonequals \exp(x_1) * \sin^2x_2$,
and translate it to it's computational counterpart in python with one-dimensional `Tensor`s:
```python
import picograd as pg

def f(x1: pg.Tensor, x2: pg.Tensor) -> pg.Tensor:
  a = pg.exp(x1)
  b = pg.sin(x2)
  c = b**2
  d = a*c
  return d
```
<small>Figure 1. Python source for the function $f: \reals, \reals \to \reals$ where $f \colonequals \exp(x_1) * \sin^2x_2$</small>

Here we've broken up the function to render the subexpressions more clearly.
But this isn't necessary — automatic differentiation will work if the function was expressed in one line.
In part one, the development of `picograd` followed that of [`numpy`](https://numpy.org/) —
an array programming language similar to Matlab but embedded in the host language of Python,
that could evaluate functions of the form $f(\mathbf{x})$
where `Tensor` objects stored their values $f(\mathbf{x})$ with the `value:` field
and the function types that produced their values with `Op`.
For instance, evaluating the specified function `f` from above with 9 and 10

```python
if __name__ == "__main__":
  print(f(9, 10))
```

populates the `Tensor.value` fields.
In part one of the book we verified this with a REPL-interface,
but we can also represent the entire expression being evaluated with a
graph of vertices and edges where the vertices are `Tensors`
(along with their `Op`s and `value`s) and the edges are their data dependencies:

Here you can see that even if the function was specified in one line,
the graph of the expression always parses into `Tensor` vertices, and data dependency edges.
You may have noticed the `Tensor.grad` fields, which supposedly store the values of derivatives $f'(\mathbf{x})$.
The question now remains in how to populate these fields.

Taking a step back to differential calculus,
deriving the derivative of $f \colonequals \exp(x_1) * \sin^2x_2$
involves the application of the chain rule where $f(x) := g(h(x)) \implies f'(x) = g'(h(x))h'(x)$.
Evaluating the derivative of the function with respect to its inputs $f'(x_1)$ and $f'(x_2)$ results in

$$f'(x_1) = \frac{\partial}{\partial x_1} \lbrack \exp(x_1) * \sin^2x_2 \rbrack
                                  = \frac{\partial}{\partial x_1} \lbrack \exp(x_1)\rbrack * \sin^2x_2
                                  = \exp(x_1)(1)*(\sin^2x_2)$$
$$f'(x_2) = \frac{\partial}{\partial x_2} \lbrack \exp(x_1) * \sin^2x_2 \rbrack
                                  = \exp(x_1) * \frac{\partial}{\partial x_2} \lbrack \sin^2x_2 \rbrack
                                  = \exp(x_1)* 2\sin x_2\cos x_2$$

symbolic and numeric differentiattion
symbolic differentiation has *performance* issues since a large unrolled expression must be constructed in order to differentiate[^0],
whereas numerical differentiation has *correctness* issues since evaluating finite differences requires evaluating functions to a precision point resulting in numerical instability.
(trace through EXAMPLE for both. talking nets widrow)


To populate the `Tensor.grad` fields, the simplest idea would be
to literally translate the manual derivation of the derivative into code.
The translation from math to code involves a design decision:
*should we evaluate from outputs to inputs (symbolically outside-in, graphically right-to-left) or from inputs to outputs (symbolically inside-out, graphically left-to-right)?*
Although the former order seems more natural with symbolic expressions,
there's nothing illegal about the latter.

```python
import picograd as pg

def f(x1: pg.Tensor, x2: pg.Tensor) -> pg.Tensor:
  a = pg.exp(x1)
  b = pg.sin(x2)
  c = b**2
  d = a*c
  return d

# dict[f(x), f'(x)] of local derivatives (adjoints)
dd_da, dd_dc = [c, a] # d(a,c):=a*c ==> d'(a)=c, d'(c)=a
da_dx1 = pg.exp(x1) # a(x1):=exp(x1) ==> a'(x1)=exp(x1)
dc_db = 2*b # c(b):=b^2 ==> c'(b)=2b
db_dx2 = pg.cos(x2) # b(x2):=sin(x2) ==> b'(x2)=cos(x2)

# outputs to inputs: outside-in symbolically, right-to-left graphically
dd_dd = pg.Tensor(1) # base case
dd_da, dd_dc = [dd_dd*dd_da, dd_dd*dd_dc]
dd_dx1 = dd_da*da_dx1 # DONE for the x1->d path

dd_db = dd_dc*dc_db
dd_dx1 = dd_db*db_dx2 # DONE for x2->path

# inputs to outputs: inside-out symbolically, left-to-right graphically
dx1_dx1, dx2_dx2 = [pg.Tensor(1), pg.Tensor(1)] # base case
da_dx1 = da_dx1*dx1_dx1
dd_dx1 = dd_da*da_dx1 # DONE for the x1->d path

db_dx2 = db_dx2*dx2_dx2
dc_dx2 = dc_dc*db_dx2
dd_dx2 = dd_dc*dc_dx_2 # DONE for the x2->d path
```

*Do you notice any difference in the number of evaluations between the two orders?*

The outputs-to-input ordering takes 6 arithmetic operations (including the destructuring),
whereas the input-to-output ordering take 7 arithmetic operations.
This is because the former can *reuse* `dd_dd`
as a dynamic programming solution to a subproblem for the two inputs, whereas the latter cannot.
And taking a step back, we only want to reuse the output because the shape of the function
is of $f:\reals^2 \to \reals$. Alternatively, if $f$ had type $f:\reals \to \reals^2$,
then the input-to-output ordering would be able to reuse results.
This distinction is referred to as "forward-mode" vs "reverse-mode", and reflects
the fact that for some function $f:\reals^n \to \reals^m$ 
the time complexity of forward-mode differentiation is proportional to $n$,
whereas that of forward-mode differentiation is proportional to $m$.
If the expression graph *fans-in* so that $n > m$, reverse-mode is preferred.
If the expression graph *fans-out* so that $m > n$, forward-mode is preferred.
However, if we take a step with a graph-theory lens, we can see that the derivative is the *sum of paths*,
where each path is a product of local derivatives from the input source to the output sink.
From a combinatorics perspective, we are calculating all the possible (ors) ways (ands)
on how the inputs perturb the output. That is:

$$\frac{\partial f}{\partial x_i } = \sum\prod \frac{\partial v_j}{\partial v_k} $$

and as long as the operations along this path are *associative* — $(AB)C = A(BC)$ —
then we can choose the order in how we perform these path products to minimize the number of operations.
Finding the optimal ordering is an NP-hard problem because ____.
For instance, if the expression graph is diamond-shaped, evaluating the derivative
with forward-mode for the left-half and reverse-mode for the right-half would be more performant.
In practice, we use reverse-mode as a heuristic, since most of the functions
that are differentiated (so they can be optimized) in the field of machine learning are neural networks of the form $f:\reals^n \to \reals$

*How can we generalize this into an algorithm?*<br/>
All we need are 1. mappings from $f(\mathbf{x}) \to f'(\mathbf{x})$ and 2. a topological sort

For the derivative rules, the same way that optimizing compilers
implement an optimization "manually" once which then gets reused many times,
the authors of deep learning frameworks also implement derivatives manually
which then become reused many times through automatic differentiation.
In theory, we can differentiate any expression with f'(x) with only a few derivative
rules for addition and multiplication, but in practice most frameworks provide sugar for complex derivatives.

For topological sort, we can simply reversed the ordering produced by a depth-first-search:
```python
def toposort(self):
  order: list[Op] = []
  visited: set[Op] = set()

  def dfs(node: Op) -> None:
    if node in visited: return
    visited.add(node)
    for src in node.src: dfs(src)
    order.append(node)

  dfs(self)
  return order

class Tensor():
  def backward():
    for t in reversed(topo):
      t.backward()
```


We will now use this idea to modify the interpretation of our deep learning framework
to not only evaluate $f(\mathbf{x})$, but $f'(\mathbf{x})$ as well. This is done
by dynamically overloading the operators at runtime[^0] to trace the expression graph
```python
chain_rules = PatternMatcher([
  (Pattern(OpCode.MATMUL, name="input"), lambda output_grad, input: (_____,)),
  (Pattern(OpCode.MATVEC, name="input"), lambda output_grad, input: (_____,)),
  (Pattern(OpCode.RECIPROCAL, name="input"), lambda output_grad, input: (-output_grad * input * input,)),
  (Pattern(OpCode.SIN, name="input"), lambda output_grad, input: ((math.pi/2 - input.src[0]).sin() * output_grad,)),
  (Pattern(OpCode.LOG2, name="input"), lambda output_grad, input: (output_grad / (input.src[0] * math.log(2)),)),
  (Pattern(OpCode.EXP2, name="input"), lambda output_grad, input: (input * output_grad * math.log(2),)),
  (Pattern(OpCode.SQRT, name="input"), lambda output_grad, input: (output_grad / (input*2),)),
  (Pattern(OpCode.ADD), lambda output_grad: (1.0*output_grad, 1.0*output_grad)),
  (Pattern(OpCode.MUL, name="input"), lambda output_grad, input: (input.src[1]*output_grad, input.src[0]*output_grad)),
])

class Tensor:
  def _forward(self, f:Callable, *other:Tensor) -> Tensor: #extra_args=(), **kwargs)
    out_tensor = evaluator.eval_uop([self, other], out_uop)

  def backward(self, grad:Tensor|None=None) -> Tensor:
    """
    backward performs by collecting tensors, computing gradients with automatic differentiation, and updating said tensors.
    """
    # 1. collect all tensors that requires grad by topologically sorting the graph of uops and filter
    all_uops = self.uop.toposort()
    tensors_require_grad: list[Tensor] = [t for tref in all_tensors if (t:=tref()) is not None and t.uop in all_uops and t.requires_grad]
    uops_require_grad = [t.uop for t in tensors_require_grad]
    assert grad is not None or self.shape == tuple(), "when no gradient is provided, backward must be called on a scalar tensor"
    if not (self.is_floating_point() and all(t.is_floating_point() for t in tensors_require_grad)): raise RuntimeError("only float Tensors have gradient")
    
    # 2. compute the gradient with a map of tensors to partials
    if grad is None: grad = Tensor(1.0, dtype=self.dtype, device=self.device, requires_grad=False) # base case is 1.0
    tens2grads = Tensor._automatically_differentiate(self.uop, grad.uop, set(uops_require_grad)) # skipping materializing zerod grads for now
    grads = [Tensor(g, device=t.device) for t,g in zip(tens2grads.keys, tens2grads.values)] # initialize tensor grads on device
    
    # 3. update the tensors that require grad with the gradient's partials
    for t,g in zip(tensors_require_grad, grads):
      assert g.shape == t.shape, f"grad shape must match tensor shape, {g.shape!r} != {t.shape!r}"
      t.grad = g if t.grad is None else (t.grad + g) # accumulate if t.grad exists
    return self

  @staticmethod
  def _automatically_differentiate(root:Op, root_grad:Op, targets:set[Op]) -> dict[Op, Op]:
    """
    _differentiate backpropagates partials on a topologically sorted expression graph with the chain rule
    and produces the gradient in the form of a map of ops to their partials (which, in turn, are ops)
    """
    tens2grads = {root: root_grad}

    # 1. topological sort
    in_target_path: dict[Op, bool] = {}
    for u in root.toposort(): in_target_path[u] = any(x in targets or in_target_path[x] for x in u.src)
    dfs = list(root.toposort()) # lambda node: node.op not in {OpCode.DETACH, OpCode.ASSIGN} and in_target_path[node])) # don't flow through DETACH/ASSIGN or anything not in target path

    # 2. backpropagation with the chain rule
    for tensor in reversed(dfs):
      if tensor not in tens2grads: continue

      local_grads: tuple[Op|None, ...]|None = cast(tuple[Op, ...]|None, chain_rules.rewrite(tensor, ctx=tens2grads[tensor]))
      if local_grads is None: raise RuntimeError(f"failed to compute gradient for {tensor.op}\n\nin {str(tensor)[0:1000]}...")
      assert len(local_grads) == len(tensor.src), f"got {len(local_grads)} gradient, expected {len(tensor.src)}"

      for tensor,local_grad in zip(tensor.src, local_grads): # <--------------------- MOOOSE: why are we accumulating inside ad()? don't we do it in backward()??
        if local_grad is None: continue
        if tensor in tens2grads: tens2grads[tensor] = tens2grads[tensor] + local_grad # accumulate if tensor exists
        else: tens2grads[tensor] = local_grad # o/w initialize
```

To implement automatic differentiation with `Tensor.backward()`,
there is a design decision to be made
— the choice of implementing it dynamically or just-in-time[^3],
similar to the decision of how to implement types for general programming languages[^4].
This stands in contrast to the alternative of performing a just-in-time, source-to-source transformation.

Let's now move onto automatically differentiating the functions of neural networks,
specifically the FFN language model from earlier. (johnson/ryan adams ordering)
n^2 vs n^3 

<iframe src="https://videolectures.net/embed/videos/deeplearning2017_johnson_automatic_differentiation?part=1" width="100%" frameborder="0" allowfullscreen style="aspect-ratio:16/9"></iframe>

### 2.2.4 Iterative Optimization with Stochastic Gradient Descent `optim.sgd`
<iframe width="560" height="315" src="https://www.youtube.com/embed/NrO20Jb-hy0?si=BMJdZzbqePV3_LNj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/Ilg3gGewQ5U?si=fn3KTCaDJHmQFJsd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/tIeHLnjs5U8?si=_DkzRHubxKOI4kW1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/NrO20Jb-hy0?si=4moCoOcWYW9ZE8g6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 2.2.5 Programming Massively Parallel Processors
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/YoKpLBeRVWY?si=k9RJbNHUpSDjQr9L" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

https://arxiv.org/pdf/1410.0759

https://arxiv.org/pdf/1804.06826
https://arxiv.org/pdf/2512.02189v1
https://girl.surgery/bad_paper
https://www.arxiv.org/pdf/2512.07004
### 2.2.6 Accelerating `GEMM` from `CUDA Rust` to `PTX`
### 2.2.7 Accelerating `GEMM` on GPU with Data Reuse
### 2.2.8 Accelerating `GEMM` on GPU with Scheduling
### 2.2.9 Accelerating `GEMM` on GPU with Tensor Cores

<!-- So far we've been training feedforward neural networks using the magical
`Tensor.backward()` function in order to materialize the gradient of the loss on our
parameter weights $w = (W^{(0)}, \dots , W^{(l-1)})$ which gradient descent uses in it's update rule
$\theta^{t+1} \coloneqq \theta^{t} - \alpha\nabla\mathscr{L}(\theta)$.
We will now dive deeper into how `.backward()` is implemented. -->
<!-- <iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/hvVO997j1ds?si=65tlIVC85ChtQDgR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->

<!-- <iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/_DYbj3n21S8?si=4-WY0ISBHsp5j3Qb" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->
<!-- <iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/NgrCQcU0Sbg?si=MjZx5af3IHOmnNKi" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/9xMQ3tLoWMo?si=GU6FQ8IWs9HZRbhR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->
<!-- <iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/GclTQbHpL6g?si=Wus_92-bytEkmIST" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->
<!-- <iframe width="698" height="393" loading="lazy" src="https://www.youtube.com/embed/ErTmTCRP1_U?si=RhdI2kT4ZdjOQeDB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->